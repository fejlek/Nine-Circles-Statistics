# Nine Circles of Statistics

The primary focus of this repository is *statistical models*, i.e., models such as linear regression, contingency tables, or survival models (I plan covering nine models at the time of writing this introduction). I will demonstrate their usage on datasets taken from Kaggle (https://www.kaggle.com). Even though each problem will be mainly focused on one particular model, I expect that due to dealing with more or less realistic datasets, I will have to cover other additional topics as the need emerges, e.g.,  dealing with missing data (multiple imputation) and dealing with dependent/correlated observations. 


The main motivation behind this work is personal: learning, practice, future reference, demonstration of skill, and, last but not least, fun. Still, I think that these small projects can be useful for any reader interested in statistical modelling. Kaggle users usually provide their solutions based on machine learning techniques, so these solutions can be unique, at least in this regard. 

## Machine learning and statistical models

Machine learning models such as random forests, gradient boosted trees, and neural networks provide very flexible classes of predictive models that can easily incorporate nonlinear dependencies and high-order interactions. There is a price to be paid, however. Machine learning models are by and large black-box models that are difficult to interpret (e.g., it is difficult to quantify the effects of interactions between predictors). They require large datasets to provide stable and reliable results (whereas regression models can be used for moderately sized datasets). In addition, the validation of these models is often based on an evaluation of the predictive performance of the model on a testing dataset (data that were held out during the construction of the model). This puts a tremendous pressure on the test data in terms of its representativnes of a new uknown data, which if insufficient, can lead to significantly inflated predictive performance of the model (e.g., test data contains the information about the same individuals as training data, but the goal is to generalize the model to the whole uknown population). One must also be careful not to accidentally *leak* the testing data into the learning process (e.g., through data preprocessing or imputation), which can again overly inflate the estimated predictive performance.

Statistical models are much easier to interpret. One can straightforwardly quantify individual main effects and effects of nonlinearities and interactions. In addition, provided that the choice model is correctly specified, the uncertainty in the parameter estimates and the model's predictive performance can be estimated using probability theory. Thus, one can use the whole dataset to learn and validate the model. This is important in cases when datasets are relatively small (e.g., clinical trials), thus splitting the data would be too wasteful. In addition, since the models are much more constrained in their form: mostly limited to linear effects with some main effect nonlinearities and low-level interactions, the dataset can be generally much smaller to obtain a stable model. 

However, greater care must be taken in the model's specification, since all nonlinearities and interactions must be exactly prespecified (e.g., using prior problem knowledge). In addition, one must consider the structure of the data to obtain valid inference, e.g., whether all observations are truly independent or if there is any correlation between observations (e.g., clustered data or panel data). And suppose there is such a structure in the data, one must choose how to model it (e.g., through random effects or by directly estimating an appropriate correlation matrix via weighted least squares). Improper model specifications can lead to significant biases in both parameter estimates and estimates of the error terms, leading to invalid inference. One also must take care in the overall modelling strategy, especially with respect to variable/model selection, which can easily produce invalid inference (e.g., due to using aggressive stepwise variable selection)


## Regression modeling strategies

In the solutions provided here, I will (or at least try to) follow  these steps that help to obtain valid statistical inference and help to obtain models that  perform well overall (inspired by *F. Harrell. Regression modeling strategies. New York: Springer-Verlag, 2001.*)

1. Data exploration (checking for missing/nonsensical values of predictors, redundancy analysis of predictors, elimination and/or grouping of predictors if needed)
2. Formulating hypotheses and the corresponding full model (main linear effects, nonlinear effects, interactions) based on the nature of the problem and effective sample size (rule of thumb: 10-20 independent observations per parameter). We should not use the predicted values (formally or informally) in model selection.
3. Single or multiple imputation (rule of thumb: multiple imputation should be used if the proportion of observations with missing values is greater than 3%). We should use a multiple imputation model that is at least as general as our full model.
4. Fitting a full model.
5. Very limited model simplification by testing the significance of *all* interaction and/or *all* nonlinear terms (rule of thumb: provided that the corresponding statistics have a P-value greater than 0.2, it should be safe to simplify the model by deleting all corresponding terms). Other than that, we should not remove any seemingly nonsignificant effects from the model.
6. Checking the distributional assumptions (usually by analyzing the residuals) and the presence of overly influential observations. We change the model if needed to obtain a valid inference.
7. Model interpretation,  hypotheses testing, and/or evaluation of predictive performance.
8. Model validation (usually via bootstrap, i.e., repeating at least 4 and 7 for each bootstrap sample). If we performed multiple imputation, we should repeat 3 for each bootstrap sample. If we performed variable selection beyond what is suggested in 5, we should repeat the variable selection step for each bootstrap sample.
   
