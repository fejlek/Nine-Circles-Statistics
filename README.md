# Nine Circles of Statistical Modeling

The primary focus of this repository is *statistical models*, i.e., models such as linear regression, contingency tables, or survival models (I plan to cover nine models at the time of writing this introduction). I will demonstrate their usage on datasets taken from Kaggle (https://www.kaggle.com) and other places such as various *R* packages . Even though each problem will be mainly focused on one particular model, I expect that due to dealing with more or less realistic datasets, I will have to cover other additional topics as the need emerges, e.g.,  dealing with missing data (multiple imputation) and dealing with dependent/correlated observations. 


The main motivation behind this work is personal: learning, practice, future reference, and, last but not least, fun. Still, I think that these small projects can be useful for any reader interested in statistical modeling. Kaggle users usually provide their solutions based on machine learning techniques, so these solutions can be unique, at least in this regard. 

## Machine learning and statistical models

Machine learning models such as random forests, gradient-boosted trees, and neural networks provide very flexible classes of predictive models that can easily incorporate nonlinear dependencies and high-order interactions. There is a price to be paid, however. Machine learning models are, by and large, black-box models that are difficult to interpret (e.g., it is difficult to quantify the effects of interactions between predictors). They require large datasets to provide stable and reliable results (whereas regression models can be used for moderately sized datasets). Their highly flexible nature can easily lead to overfitting (fitting the noise in the particular dataset instead of the underlying structure of the data). Serious overfitting can easily lead to poor generalization of the model beyond the original dataset. Statistical inference, such as variable importance and hypothesis testing, is possible in the context of machine learning models, but it is quite complex and often computationally expensive. 

Machine learning algorithms involve hyperparameters that aim to mitigate overfitting. However, these values must be selected individually for a particular dataset, and thus, their values are usually optimized during the learning process of the model. The evaluation of the performance of a given model is often purely based on a direct evaluation of a testing dataset (data that were held out during the construction of the model), and this process is often repeated for different train/test data splits, leading to so-called cross-validation. Thus, hyperparameter selection can be quite computationally expensive. Moreover, to validate the whole procedure, including hyperparameter selection, and obtain unbiased estimates of the expected predictive performance, one would need to consider another cross-validation on top of the whole procedure: *nested cross-validation.* 

Statistical models are much easier to interpret. One can straightforwardly quantify individual main effects and effects of nonlinearities and interactions. In addition, provided that the choice model is correctly specified, the statistical inference can be performed quite directly using asymptotic theory. One can also use the whole dataset to learn the model and then use a cross-validation (or a bootstrap) to validate the estimated predictive performance on a new dataset since there are usually no hyperparameters to tune. The fact that splitting the data is not always necessary for learning the model is important in cases when datasets are relatively small (e.g., clinical trials). In addition, since the models are much more constrained in their form, mostly limited to linear effects with some main effect nonlinearities and low-level interactions, the dataset can generally be much smaller to obtain a stable model.

However, greater care must be taken in the model's specification since all nonlinearities and interactions must be prespecified (e.g., using prior problem knowledge). In addition, one must consider the structure of the data to obtain valid inference, e.g., whether all observations are truly independent or if there is any correlation between observations (e.g., clustered data or panel data). Suppose there is such a structure in the data; one must either choose a model for such a structure (e.g., through random effects or by directly estimating an appropriate correlation matrix via weighted least squares) or use an inference approach that is robust to correlation between observations. Improper model specifications can lead to significant biases in both parameter estimates and estimates of the error terms, leading to invalid inference. One also must take care in the overall modeling strategy, especially with respect to variable selection (e.g., aggressive stepwise variable selection), which can also easily produce invalid inference and overly optimistic estimates of predictive performance.


## Regression modeling strategies

In the solutions provided here, I will (or at least try to) follow  these steps that help to obtain valid statistical inference and models that  perform well overall (inspired by *F. Harrell. Regression modeling strategies. New York: Springer-Verlag, 2001.*)

1. Data exploration (checking for missing/nonsensical values of predictors, redundancy analysis of predictors, elimination and/or grouping of predictors if needed)
2. Formulating hypotheses and the corresponding full model (main linear effects, nonlinear effects, interactions) based on the nature of the problem and effective sample size (rule of thumb: 10-20 independent observations per parameter). We should not use the predicted values (formally or informally) in model selection.
3. Single or multiple imputation (rule of thumb: multiple imputation should be used if the proportion of observations with missing values is greater than 3%). We should use a multiple imputation model that is at least as general as our full model.
4. Fitting a full model.
5. Very limited model simplification by testing the significance of *all* interaction and/or *all* nonlinear terms (rule of thumb: provided that the corresponding statistics have a P-value greater than 0.2, it should be safe to simplify the model by deleting all corresponding terms). Other than that, we should not remove any seemingly nonsignificant effects from the model. This step should be skipped if we are interested only in hypothesis testing: a full model fit will result in more accurate P-values for tests for the variables of interest.
6. Checking the distributional assumptions (e.g., by analyzing the residuals) and the presence of overly influential observations. We change the model if needed to obtain a valid inference (i.e., we must return to step 4).
7. Model interpretation, effect estimation, hypotheses testing,  and model performance measures. 
8. Model validation (of predictive accuracy) via bootstrap or cross-validation. If we performed multiple imputation, we should include step 3 in the validation. If we performed variable selection beyond what is suggested in 5, we should include step 5 in the validation. This step is not strictly necessary if we are only interested in hypothesis testing and/or effect estimation rather than prediction. However, effect estimation and hypothesis testing presented here are model-based, and thus, it is desirable to ascertain that the model used to draw the conclusions is reasonable. 
   
## Technical solution

All solutions will programmed in R and provided as R markdown documents and github markdown documents.