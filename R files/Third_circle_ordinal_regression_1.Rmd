---
title: "The Third Circle: Ordinal Regression, Part One"
author: "Jiří Fejlek"
date: "2025-07-06"
output:
  md_document:
    variant: GFM
code_folding: hide    
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

<br/>
In this project, we will model an ordinal outcome, i.e., categorical data with a natural order. We will mainly cover the ordered logit model (proportional odds logistic regression). We will also briefly cover the continuation ratio model. 

Our primary goal is to develop a model for predicting students' math final grades, but we will also be interested in identifying predictors that have the greatest impact. We will split this presentation into two parts. In the first part, we will describe the data preparation and exploration and then fit the models. In the second part, we will validate the final model obtained in the first part and discuss the results.
<br/>

## Predicting Grades for the School Year

<br/> The dataset used in this project is obtained from https://www.kaggle.com/code/janiobachmann/predicting-grades-for-the-school-year dataset, and this dataset originates from the paper *P. Cortez and A. M. Gonçalves Silva. Using data mining to predict secondary school student performance. (2008).* The data were collected during the 2005-2006 school year from two public secondary schools in the Alentejo region of Portugal. The dataset was built from two sources: school reports and questionnaires.
<br/>

* **sex** 
* **age** 
* **school** - *Gabriel Pereira* or *Mousinho da Silveira*
* **address** - student's home address type (urban or rural)
* **Pstatus** - parent's cohabitation status (together or apart)
* **Medu** - mother's education (0 – none, 1 – primary education (4th grade), 2 – 5th to 9th grade, 3 – secondary education or 4 – higher education)
* **Mjob** - mother's job (teacher, health care related, civil services (e.g., administrative or police), at home or other)
* **Fedu** - father’s education
* **Fjob** - father’s job
* **guardian** -  student’s guardian
* **famsize** - lesser than three or greater than three
* **famrel** -  from 1 – very bad to 5 – excellent
* **reason** - reason to choose this school
* **traveltime** - home to school travel time (1 – < 15 min., 2 – 15 to 30 min., 3 – 30 min. to 1 hour
or 4 – > 1 hour)
* **studytime** - weekly study time (1 – < 2 hours, 2 – 2 to 5 hours, 3 – 5 to 10 hours or 4 – > 10 hours)
* **failures** - number of past class failures (n if 1 ≤ n < 3, else 4)
* **schoolsup** - extra educational school support
* **famsup** - family educational support 
* **activities** - extra-curricular activities
* **paidclass** - extra paid classes
* **internet** - Internet access at home
* **nursery** - attended nursery school
* **higher** - wants to take higher education
* **romantic** - with a romantic relationship
* **freetime** - free time after school (from 1 – very low to 5 – very high)
* **goout** - going out with friends (from 1 – very low to 5 – very high)
* **Walc** - weekend alcohol consumption (from 1 – very low to 5 – very high)
* **Dalc** - workday alcohol consumption (from 1 – very low to 5 – very high)
* **health** - current health status (from 1 – very bad to 5 – very good)
* **absences** - number of school absences
* **G1** - first period grade
* **G2** - second period grade
* **G3** - final grade

## Initial Data Exploration

<br/>
We start with the data exploration. Let's load the dataset and take a look.
<br/>


```{r, message=FALSE}
library(readr)
student_mat <- read_csv('C:/Users/elini/Desktop/nine circles/student-mat.csv')
head(student_mat)
```

<br/>
We have 395 observations, 30 predictors, and three outcomes: the first-period grade, the second-period grade, and the final grade. In this project, we will model the final grade using 30 predictors (excluding the period grades in the model). 

First, we convert the final grade via the Erasmus grade conversion system. Then, we will split the grade into three categories (A and B, C and D, and F) obtaining **grade** to obtain the final ordinal outcomes. We will assume just three ordinal categories mostly for simplicity's sake. However, another reason is to get more observations in each category, which allows us to more easily fit "partial" ordinal models as we will see later. 
<br/>

```{r, message=FALSE}
library(tibble)
library(dplyr)

G3 <- student_mat$G3
erasmus_grade <- factor(case_when(G3 > 15 ~ 'A', G3 > 13 & G3 < 16 ~ 'B',  G3 > 11 & G3 < 14 ~ 'C',  G3 > 9 & G3 < 12 ~ 'D' , G3 < 10 ~ 'F'))
erasmus_grade <- factor(erasmus_grade, ordered = TRUE, levels=rev(levels(erasmus_grade)))


grade <- factor(case_when(G3 > 13 ~ 'A/B',  G3 > 9 & G3 < 14 ~ 'C/D', G3 < 10 ~ 'F'))
grade <- factor(grade, ordered = TRUE, levels=rev(levels(grade)))

par(mfrow = c(1, 2))
plot(erasmus_grade)
plot(grade)
```

<br/>
Let's check if any data is missing.
<br/>

```{r}
any(duplicated(student_mat))
any(is.na(student_mat))
```

<br/>
Next, we convert the variables to the correct types.
<br/>

```{r}
student_mat$school  <- factor(student_mat$school)
student_mat$sex  <- factor(student_mat$sex)
student_mat$address  <- factor(student_mat$address)
student_mat$famsize   <- factor(student_mat$famsize)
student_mat$Pstatus   <- factor(student_mat$Pstatus)
student_mat$Medu     <- factor(student_mat$Medu, ordered = TRUE)
student_mat$Fedu     <- factor(student_mat$Fedu, ordered = TRUE)
student_mat$Mjob   <- factor(student_mat$Mjob)
student_mat$Mjob <- relevel(student_mat$Mjob, ref = 'other')
student_mat$Fjob  <- factor(student_mat$Fjob)
student_mat$Fjob <- relevel(student_mat$Fjob, ref = 'other')
student_mat$reason  <- factor(student_mat$reason)
student_mat$reason <- relevel(student_mat$reason, ref = 'other')
student_mat$guardian   <- factor(student_mat$guardian)
student_mat$guardian <- relevel(student_mat$guardian, ref = 'other')
student_mat$traveltime  <- factor(student_mat$traveltime, ordered = TRUE)
student_mat$studytime   <- factor(student_mat$studytime, ordered = TRUE)
student_mat$schoolsup  <- factor(student_mat$schoolsup)
student_mat$famsup  <- factor(student_mat$famsup)
student_mat$paid <- factor(student_mat$paid)
student_mat$activities <- factor(student_mat$activities)
student_mat$nursery <- factor(student_mat$nursery)
student_mat$higher <- factor(student_mat$higher)
student_mat$internet <- factor(student_mat$internet)
student_mat$romantic <- factor(student_mat$romantic)
student_mat$famrel <- factor(student_mat$famrel, ordered = TRUE)
student_mat$freetime <- factor(student_mat$freetime, ordered = TRUE)
student_mat$goout <- factor(student_mat$goout, ordered = TRUE)
student_mat$Dalc <- factor(student_mat$Dalc, ordered = TRUE)
student_mat$Walc <- factor(student_mat$Walc, ordered = TRUE)
student_mat$health <- factor(student_mat$health, ordered = TRUE)
```

<br/>
Let us check the predictors.
<br/>

```{r,echo=FALSE}
par(mfrow = c(1, 2))
plot(student_mat$school,xlab = 'school')
plot(student_mat$sex,xlab = 'sex')
plot(student_mat$address,xlab = 'address')
plot(student_mat$famsize,xlab = 'famsize')
plot(student_mat$Pstatus,xlab = 'Pstatus')
plot(student_mat$Medu,xlab = 'Medu')
plot(student_mat$Fedu,xlab = 'Fedu')
plot(student_mat$Mjob,xlab = 'Mjob')
plot(student_mat$Fjob,xlab = 'Fjob')
plot(student_mat$reason,xlab = 'reason')
plot(student_mat$guardian,xlab = 'guardian')
plot(student_mat$traveltime,xlab = 'traveltime')
plot(student_mat$studytime,xlab = 'studytime')
plot(student_mat$schoolsup,xlab = 'schoolsup')
plot(student_mat$famsup,xlab = 'famsup')
plot(student_mat$paid,xlab = 'paid')
plot(student_mat$activities,xlab = 'activities')
plot(student_mat$nursery,xlab = 'nursery')
plot(student_mat$higher,xlab = 'higher')
plot(student_mat$internet,xlab = 'internet')
plot(student_mat$romantic,xlab = 'romantic')
plot(student_mat$famrel,xlab = 'famrel')
plot(student_mat$freetime,xlab = 'freetime')
plot(student_mat$goout,xlab = 'goout')
plot(student_mat$Dalc,xlab = 'Dalc')
plot(student_mat$Walc,xlab = 'Walc')
plot(student_mat$health,xlab = 'health')
```

<br/>
All predictors seem reasonable enough. However even though we have merely 30 predictors, almost all of them are categorical (nominal or ordinal). Thus, the number of parameters in the model is much greater.
<br/>

```{r}
dim(model.matrix(erasmus_grade ~. - G1 - G2 - G3 , data = student_mat))
```

<br/>
We see that our model would have 68 parameters (plus thresholds and possible non-parallel terms as we will see later). The effective sample size for the ordinal response under proportional odds assumption (which states in short that the effect of variables is independent of the response level, i.e., no non-parallel terms in the model) is $n - \frac{1}{n^2}\sum_i n_i^3$ (*F. E. Harrell. Regression modeling strategies: with applications to linear models, logistic regression, and survival analysis. Vol. 608. New York: springer, 2001.*).
<br/>

```{r}
summary(grade)
395 - 1/395^2*(130^3+165^3+100^3)
```

<br/>
We see that under under proportional odds assumption , our rule of thumb suggests that a model for **grade**  could support about 17 to 34 parameters.

Now, if we were interested in testing only a specific hypothesis, the model's parsimony is not as important. However, since we are also interested in the predictive performance of our model, 68 parameters are way too much (we have a significant risk of overfitting and subsequent poor generalization of the resulting model on new data). 

Let us look for redundant variables first.
<br/>

```{r}
library(Hmisc)
redun(~.- G1 - G2 - G3 ,data = student_mat,nk = 0, r2 = 0.95)
```

<br/>
No variable seems redundant. Thus, we will need to perform the data reduction another way. Ideally, creating summarizing variables would be done with the help of experts in the particular field. In this project, we will have to do. Let's take a look at cluster analysis, which groups the predictors based on Spearman's rank correlation coefficients. For simplicity's sake, we treat the ordinal variables as numerical (by considering only linear polynomial contrasts *poly(.,1)*).
<br/>

```{r}
par(mfrow = c(1, 1))
clus <- varclus(~ school + sex + age + address + famsize + Pstatus + poly(Fedu ,1) + poly(Medu ,1) + Mjob + Fjob + reason + guardian + poly(traveltime,1) + poly(studytime,1) + failures + schoolsup + famsup + paid + activities  + nursery  + higher  + internet  + romantic + poly(famrel ,1) + poly(freetime,1) + poly(goout ,1) + poly(Dalc ,1) + poly(Walc ,1) +  poly(health ,1) + absences,data = student_mat)
plot(clus)
```

<br/>
Based on the results, we will combine *Medu* and *Fedu* (mother's and father's education) into one summary score. We will do the same for *Dalc* and *Walc* (workday and weekend alcohol consumption), *famsup*, and *paid* (family support and extra paid classes). We will also simplify *Mjob* and *Fjob* by merely tracking whether either parent is a teacher, healthcare-related, in civil services, or at home. 
<br/>

```{r}
# Edu
edu <- factor(round((as.numeric(student_mat$Medu) + as.numeric(student_mat$Fedu))/2), ordered = TRUE)  

# Alc
alc <- factor(round((as.numeric(student_mat$Dalc) + as.numeric(student_mat$Walc))/2), ordered = TRUE)

# Extra support
extrasup <- factor(student_mat$famsup == 'yes' | student_mat$paid == 'yes')
levels(extrasup) <- c('no','yes')

# Jobs
at_home <- factor(student_mat$Fjob == 'at_home' | student_mat$Mjob == 'at_home')
levels(at_home) <- c('no','yes')

health <- factor(student_mat$Fjob == 'health' | student_mat$Mjob == 'health')
levels(health) <- c('no','yes')

services <- factor(student_mat$Fjob == 'services' | student_mat$Mjob == 'services')
levels(services) <- c('no','yes')

teacher <- factor(student_mat$Fjob == 'teacher' | student_mat$Mjob == 'teacher')
levels(teacher) <- c('no','yes')
```

<br/>
Lastly, we choose to remove the variables *guardian* and *reason* from the model (we suppose these variables are probably not that important for predicting the final grades) and the variable *freetime* (we think its effect is covered in the model by variables *studytime*,*traveltime* and *goout*).
<br/>

```{r}
student_mat_final <- student_mat

student_mat_final <- subset(student_mat_final,select = -c(G1,G2,G3, Medu,Fedu,Mjob,Fjob,reason,guardian,famsup,paid,freetime,Dalc,Walc))

student_mat_final <- student_mat_final %>% mutate(edu = edu) %>% mutate(alc = alc) %>% mutate(extrasup = extrasup) %>% mutate(at_home = at_home) %>% mutate(health = health) %>% mutate(services = services) %>% mutate(teacher = teacher) %>% mutate(grade = grade)
```

<br/>
Thus, our final full model has 44 parameters corresponding to the prediction variables (we will consider no interactions) and we can consider further parameter reduction by lowering the maximum degrees of polynomial contrasts. We should note that there will also be additional parameters corresponding to thresholds and non-parallel terms, as we will see shortly. 
<br/>

```{r}
dim(model.matrix(grade ~ school + sex + age + address + famsize + Pstatus + traveltime + studytime + failures + schoolsup + activities + nursery  + higher  + internet  + romantic + famrel  + goout + health + absences + edu + alc +  extrasup + at_home + services + teacher, data = student_mat_final))
```

## Ordered logit model (proportional odds logistic regression) 

<br/>
To model the ordinal response (in this case, the final grade), we will use the ordered logit model. Ordered logit models cumulative distribution functions $P[Y \leq k ] = \mathrm{ilogit}\, (\theta_k - X\beta)$ for ordinal response $k = 1,2,3, \ldots$. The model is called proportional since the parameter $\beta$ does not depend on class $k$.

We fit the ordered logit model using the *polr* function from the *MASS* package.
<br/>

```{r,message = FALSE,warning=FALSE}
library(MASS)
library(lmtest)

full_model <- polr(grade ~ school + sex + age + address + famsize + Pstatus + traveltime + studytime + failures + schoolsup + activities + nursery + higher + internet + romantic + famrel + goout + health + absences + edu + alc + extrasup + at_home + services + teacher, data = student_mat_final)

# Coefficients
coeftest(full_model)

# Confidence intervals (profile likelihood)
confint(full_model)
```

<br/>
Since we assume no significant interactions, we can use type II ANOVA to test the main effects in the model.
<br/>

```{r,message = FALSE,warning=FALSE}
library(car)
Anova(full_model)
```

<br/>
We see that somewhat significant predictors in the full model (p-value < 0.1) are **sex**, **age**, **studytime**, **failures**, **schoolsup**, **health**, **edu**, and **teacher**.

Now, let us take a look at the predictions. We first compute the linear predictor and then obtain the cumulative probability (distribution function). Class probabilities are derived from cumulative probabilities quite straightforwardly.
<br/>

```{r,message = FALSE,warning=FALSE}
library(faraway)

# linear predictor (i.e., Xbeta)
model.matrix(full_model)[1:3,2:44] %*% (coefficients(full_model))
# or simply
full_model$lp[1:3]

# compute the cumulative probability
prob <- matrix(NA,5,2)
for(i in 1:5){
prob[i,1:2] <- ilogit(full_model$zeta - full_model$lp[i])
}
# compute class probabilities
prob <- cbind(prob[,1],prob[,2]-prob[,1],1-prob[,2])
colnames(prob) <- c('F', 'C/D', 'A/B')
prob

# or simply
predict(full_model, type = 'probs')[1:5,]
```

<br/>
Unfortunately, the function *predict* for the *polr* model does not provide confidence intervals. Probably the most straightforward way to get these is a percentile-based confidence interval based on a simple nonparametric (pairs) bootstrap. For example, the confidence intervals for predicted probabilities for the first observation are as follows.
<br/>

```{r,message = FALSE,warning=FALSE}
set.seed(123) # for reproducibility
nb <- 2500
probmat <- matrix(NA,nb,3)

colnames(probmat) <- c('F', 'C/D', 'A/B')

for(i in 1:nb){

  student_mat_final_new <-  student_mat_final[sample(nrow(student_mat_final) , rep=TRUE),]
  
  # we skip the iterations in which fit failed to converge
  
  full_model_new <- tryCatch(polr(grade ~ school + sex + age + address + famsize + Pstatus + traveltime + studytime + failures + schoolsup + activities + nursery + higher + internet + romantic + famrel + goout + health + absences + edu + alc + extrasup + at_home + services + teacher, data = student_mat_final_new), error = function(e) {NaN})
  probmat[i,] <- tryCatch({predict(full_model_new, student_mat_final[1,], type = 'probs')}, error = function(e) {NaN})
  
}

boot_ci <- t(apply(probmat,2,function(x) quantile(x[!is.na(x)],c(0.025,0.975))))
boot_ci
```

<br/>
Next, let us plot the predicted probabilities vs. individual predictors using *sjPlot*. We recomputed the model using the *ordinal* package since *sjPlot* supports it better than *polr* (for which it does not compute confidence intervals).
<br/>

```{r,message = FALSE,warning=FALSE}
library(sjPlot)
library(ordinal) 

full_model_clm <- clm(grade ~ school + sex + age + address + famsize + Pstatus + traveltime + studytime + failures + schoolsup + activities + nursery + higher + internet + romantic + famrel + goout + health + absences + edu + alc + extrasup + at_home + services + teacher, data = student_mat_final)
```

```{r,echo=FALSE,message = FALSE,warning=FALSE}
plot_model(full_model_clm, type = "pred", terms = c('school'))
plot_model(full_model_clm, type = "pred", terms = c('sex'))
plot_model(full_model_clm, type = "pred", terms = c('age'))
plot_model(full_model_clm, type = "pred", terms = c('address'))
plot_model(full_model_clm, type = "pred", terms = c('famsize'))
plot_model(full_model_clm, type = "pred", terms = c('Pstatus'))
plot_model(full_model_clm, type = "pred", terms = c('traveltime'))
plot_model(full_model_clm, type = "pred", terms = c('studytime'))
plot_model(full_model_clm, type = "pred", terms = c('failures'))
plot_model(full_model_clm, type = "pred", terms = c('schoolsup'))
plot_model(full_model_clm, type = "pred", terms = c('activities'))
plot_model(full_model_clm, type = "pred", terms = c('higher'))
plot_model(full_model_clm, type = "pred", terms = c('internet'))
plot_model(full_model_clm, type = "pred", terms = c('romantic'))
plot_model(full_model_clm, type = "pred", terms = c('famrel'))
plot_model(full_model_clm, type = "pred", terms = c('goout'))
plot_model(full_model_clm, type = "pred", terms = c('health'))
plot_model(full_model_clm, type = "pred", terms = c('absences'))
plot_model(full_model_clm, type = "pred", terms = c('edu'))
plot_model(full_model_clm, type = "pred", terms = c('alc'))
plot_model(full_model_clm, type = "pred", terms = c('extrasup'))
plot_model(full_model_clm, type = "pred", terms = c('at_home'))
plot_model(full_model_clm, type = "pred", terms = c('services'))
plot_model(full_model_clm, type = "pred", terms = c('teacher'))
```

<br/>
From the plots, we see that the model predicts that **males** perform slightly better. In addition, the predicted probabilities of the F grade increases increases with **schoolsup**, **failures**, and **age**. **studytime**, and a low number of **absences** also seems to improve the grades a bit. 

Let us check the model assumptions. Similarly to logistic regression, the ordered logit directly models the class probabilities. The assumption concerning the probabilities is *proportional odds assumption*:  $\mathrm{logit}\; P(Y <=j|X_1)$ - $\mathrm{logit}\; P(Y <=j|X_2) = (X_2-X_1)\beta$, i.e, the effect of $X$ on relative odds does not depend on class $j$ since $\beta$ are independent of class.

Let us check the reasonability of the proportional odds assumption.First, we use the function *plot.xmean.ordinaly* from the *rms* package, which plots the observed means of predictors versus levels of Y and the estimated expected means of predictors under the proportional odds assumption.
<br/>

```{r,message = FALSE,warning=FALSE}
library(rms)
par(mfrow = c(1, 2))
plot.xmean.ordinaly(grade ~ school + sex + age + address + famsize + Pstatus + traveltime + studytime + failures + schoolsup + activities + nursery  + higher  + internet  + romantic + famrel + goout + health + absences + edu + alc +  extrasup + at_home + services + teacher, student_mat_final,cr=FALSE)
```

<br/>
We see that the proportional odds assumption seem reasonable for many predictors but it also may be violated for some others. Another quick test is to look at coefficients logistic regression for events $Y \leq k$, where $Y$ is the ordinal outcome. Provided that the proportional odds assumptions holds these coefficients should be similar. 
<br/>

```{r,message = FALSE,warning=FALSE}
logit1 <- glm(grade <= 'F' ~ school + sex + age + address + famsize + Pstatus + traveltime + studytime + failures + schoolsup + activities + nursery + higher + internet + romantic + famrel + goout + health + absences + edu + alc + extrasup + at_home + services + teacher, family = binomial ,data = student_mat_final)
logit2 <- glm(grade <= 'C/D' ~ school + sex + age + address + famsize + Pstatus + traveltime + studytime + failures + schoolsup + activities + nursery + higher + internet + romantic + famrel + goout + health + absences + edu + alc + extrasup + at_home + services + teacher, family = binomial ,data = student_mat_final)


logitmodels <- cbind(coefficients(logit1),sqrt(diag(vcov(logit1))),coefficients(logit2),sqrt(diag(vcov(logit2))))
colnames(logitmodels) <- c('<=F', 'Std. Error', '<= C/D', 'Std. Error')
round(logitmodels,4)
```

<br/>
One thing to notice is that some coefficients such as **higher** are very hard to estimate since almost all values of **higher** for the A/B grade are *yes*. Consequently, we cannot really decide from the data whether the proportional odds assumption is met for **higher**. We observe a similar problem for **alc** and **traveltime**.
<br/>

```{r,message = FALSE,warning=FALSE,echo=FALSE}
par(mfrow = c(1, 1))
plot(student_mat_final$grade,student_mat_final$higher,xlab = 'higher')
plot(student_mat_final$grade,student_mat_final$alc,xlab = 'alc')
plot(student_mat_final$grade,student_mat_final$traveltime,xlab = 'traveltime')
```

<br/>
Another more formal test from *F. E. Harrell. Regression modeling strategies: with applications to linear models, logistic regression, and survival analysis. Vol. 608. New York: springer, 2001.* involves *score residual* plots.
<br/>

```{r,message = FALSE,warning=FALSE}
full_model_lrm <- lrm(grade ~ school + sex + age + address + famsize + Pstatus + traveltime + studytime + failures + schoolsup + activities + nursery  + higher  + internet  + romantic + famrel + goout + health + absences + edu + alc +  extrasup + at_home + services + teacher, data = student_mat_final, x=TRUE , y=TRUE )

par(mfrow = c(1, 2))
resid(full_model_lrm , 'score.binary' , pl=TRUE)
```

<br/>
A confidence interval that lies outside of the zero line provides strong evidence against the proportional odds assumption. We observe that **alc** and **traveltime** are borderline.

An alternative to the ordered logit is a model where we allow $\beta$ to vary: $P[Y \leq k ] = \mathrm{ilogit}\, (\theta_k - X\beta_k)$. This model no longer assumes the proportional odds assumption. The price for the generalization is a much larger number of parameters. Thus, it is advisable to relax the proportional odds assumption only for some variables forming the so-called *partial proportional odds model*. The package *ordinal* provides the *nominal_test* function that fits the partial proportional odds model for each variable separately and compares it with the proportional odds model.
<br/>

```{r,message = FALSE,warning=FALSE}
full_model_clm <- clm(grade ~ school + sex + age + address + famsize + Pstatus + traveltime + studytime + failures + schoolsup + activities + nursery + higher + internet + romantic + famrel + goout + health + absences + edu + alc + extrasup + at_home + services + teacher, data = student_mat_final)
nominal_test(full_model_clm)
```

<br/>
We observe that partial proportional odds models **traveltime**, **schoolsup**, and **higher** are almost significant and **alc** is significant in comparison to the proportional odds model. However, we have to keep in mind our previous observation that coefficients for **traveltime**, **higher**, and **alc** were hard to estimate. We can confirm this fact by checking the convergence of the fits. 
<br/>

```{r,message = FALSE,warning=FALSE}
partial_traveltime_model <- clm(grade ~ school + sex + age + address + famsize + Pstatus  + studytime + failures + schoolsup + activities + nursery + higher + internet + romantic + famrel + goout + health + absences + edu + alc + extrasup + at_home + services + teacher, nominal = ~traveltime, data = student_mat_final)
partial_traveltime_model$convergence

partial_higher_model <- clm(grade ~ school + sex + age + address + famsize + Pstatus + traveltime + studytime + failures + schoolsup + activities + nursery + internet + romantic + famrel + goout + health + absences + edu + alc + extrasup + at_home + services + teacher,nominal = ~ higher, data = student_mat_final)
partial_higher_model$convergence

partial_alc_model <- clm(grade ~ school + sex + age + address + famsize + Pstatus + traveltime + studytime + failures + schoolsup + activities + nursery + higher + internet + romantic + famrel + goout + health + absences + edu  + extrasup + at_home + services + teacher,nominal = ~ alc, data = student_mat_final)
partial_alc_model$convergence
```

<br/>
Since **traveltime** and **alc** are ordinal variables, we can make the fit more stable by considering only polynomial contrasts of limited order. 
<br/>

```{r,message = FALSE,warning=FALSE}
partial_lin_traveltime_model <- clm(grade ~ school + sex + age + address + famsize + Pstatus  + studytime + failures + schoolsup + activities + nursery + higher + internet + romantic + famrel + goout + health + absences + edu + alc + extrasup + at_home + services + teacher, nominal = ~poly(traveltime,2), data = student_mat_final)
partial_lin_traveltime_model$convergence
anova(partial_lin_traveltime_model,full_model_clm)


partial_lin_alc_model <- clm(grade ~ school + sex + age + address + famsize + Pstatus + traveltime + studytime + failures + schoolsup + activities + nursery + higher + internet + romantic + famrel + goout + health + absences + edu + extrasup + at_home + services + teacher, nominal = ~poly(alc,3), data = student_mat_final)
partial_lin_alc_model$convergence
anova(partial_lin_alc_model,full_model_clm)
```

<br/>
We observe that non-parallel slopes for **traveltime** and especially **alc** are significant. 

Since some "non-parallel" terms seem significant, let us fit a partial proportion model and check whether are conclusion about important predictors would change. Our full model includes significant non-parallel terms (P < 0.1) with **higher** assumed to be proportional and polynomial contrasts of a limited order for **traveltime** and **alc** due to the aforementioned issues.
<br/>

```{r,message = FALSE,warning=FALSE}
full_model_clm_PP <- clm(grade ~ higher + school + sex + age + address + famsize + Pstatus  + studytime + failures  + activities + nursery  + internet + romantic + famrel + goout + health + absences + edu + extrasup + at_home + services, nominal = ~ poly(traveltime,2) + schoolsup + poly(alc,3) + teacher,data = student_mat_final)
full_model_clm_PP$convergence
summary(full_model_clm_PP)
```


```{r,message = FALSE,warning=FALSE,echo=FALSE}

school_model_clm_PP <- clm(grade ~ higher  + sex + age + address + famsize + Pstatus  + studytime + failures  + activities + nursery  + internet + romantic + famrel + goout + health + absences + edu + extrasup + at_home + services, nominal = ~ poly(traveltime,2) + schoolsup + poly(alc,3) + teacher,data = student_mat_final)

sex_model_clm_PP <- clm(grade ~ higher + school + age + address + famsize + Pstatus  + studytime + failures  + activities + nursery  + internet + romantic + famrel + goout + health + absences + edu + extrasup + at_home + services, nominal = ~ poly(traveltime,2) + schoolsup + poly(alc,3) + teacher,data = student_mat_final)

age_model_clm_PP <- clm(grade ~ higher + school + sex  + address + famsize + Pstatus  + studytime + failures  + activities + nursery  + internet + romantic + famrel + goout + health + absences + edu + extrasup + at_home + services, nominal = ~ poly(traveltime,2) + schoolsup + poly(alc,3) + teacher,data = student_mat_final)

address_model_clm_PP <- clm(grade ~ higher + school + sex + age  + famsize + Pstatus  + studytime + failures  + activities + nursery  + internet + romantic + famrel + goout + health + absences + edu + extrasup + at_home + services, nominal = ~ poly(traveltime,2) + schoolsup + poly(alc,3) + teacher,data = student_mat_final)

famsize_model_clm_PP <- clm(grade ~ higher + school + sex + age + address  + Pstatus  + studytime + failures  + activities + nursery  + internet + romantic + famrel + goout + health + absences + edu + extrasup + at_home + services, nominal = ~ poly(traveltime,2) + schoolsup + poly(alc,3) + teacher,data = student_mat_final)

Pstatus_model_clm_PP <- clm(grade ~ higher + school + sex + age + address + famsize  + studytime + failures  + activities + nursery  + internet + romantic + famrel + goout + health + absences + edu + extrasup + at_home + services, nominal = ~ poly(traveltime,2) + schoolsup + poly(alc,3) + teacher,data = student_mat_final)

traveltime_model_clm_PP <- clm(grade ~ higher + school + sex + age + address + famsize + Pstatus  + studytime + failures  + activities + nursery  + internet + romantic + famrel + goout + health + absences + edu + extrasup + at_home + services, nominal = ~ schoolsup + poly(alc,3) + teacher,data = student_mat_final)

studytime_model_clm_PP <- clm(grade ~ higher + school + sex + age + address + famsize + Pstatus  + failures  + activities + nursery  + internet + romantic + famrel + goout + health + absences + edu + extrasup + at_home + services, nominal = ~ poly(traveltime,2) + schoolsup + poly(alc,3) + teacher,data = student_mat_final)

failures_clm_PP <- clm(grade ~ higher + school + sex + age + address + famsize + Pstatus  + studytime + activities + nursery  + internet + romantic + famrel + goout + health + absences + edu + extrasup + at_home + services, nominal = ~ poly(traveltime,2) + schoolsup + poly(alc,3) + teacher,data = student_mat_final)

schoolsup_clm_PP <- clm(grade ~ higher + school + sex + age + address + famsize + Pstatus  + studytime + failures  + activities + nursery  + internet + romantic + famrel + goout + health + absences + edu + extrasup + at_home + services, nominal = ~ poly(traveltime,2)  + poly(alc,3) + teacher,data = student_mat_final)

activities_model_clm_PP <- clm(grade ~ higher + school + sex + age + address + famsize + Pstatus  + studytime + failures + nursery  + internet + romantic + famrel + goout + health + absences + edu + extrasup + at_home + services, nominal = ~ poly(traveltime,2) + schoolsup + poly(alc,3) + teacher,data = student_mat_final)

nursery_model_clm_PP <- clm(grade ~ higher + school + sex + age + address + famsize + Pstatus  + studytime + failures  + activities + internet + romantic + famrel + goout + health + absences + edu + extrasup + at_home + services, nominal = ~ poly(traveltime,2) + schoolsup + poly(alc,3) + teacher,data = student_mat_final)

higher_clm_PP <- clm(grade ~ school + sex + age + address + famsize + Pstatus  + studytime + failures  + activities + nursery  + internet + romantic + famrel + goout + health + absences + edu + extrasup + at_home + services, nominal = ~ poly(traveltime,2) + schoolsup + poly(alc,3) + teacher,data = student_mat_final)

internet_clm_PP <- clm(grade ~ higher + school + sex + age + address + famsize + Pstatus  + studytime + failures  + activities + nursery + romantic + famrel + goout + health + absences + edu + extrasup + at_home + services, nominal = ~ poly(traveltime,2) + schoolsup + poly(alc,3) + teacher,data = student_mat_final)

romantic_clm_PP <- clm(grade ~ higher + school + sex + age + address + famsize + Pstatus  + studytime + failures  + activities + nursery  + internet  + famrel + goout + health + absences + edu + extrasup + at_home + services, nominal = ~ poly(traveltime,2) + schoolsup + poly(alc,3) + teacher,data = student_mat_final)

famrel_clm_PP <- clm(grade ~ higher + school + sex + age + address + famsize + Pstatus  + studytime + failures  + activities + nursery  + internet + romantic  + goout + health + absences + edu + extrasup + at_home + services, nominal = ~ poly(traveltime,2) + schoolsup + poly(alc,3) + teacher,data = student_mat_final)

goout_clm_PP <- clm(grade ~ higher + school + sex + age + address + famsize + Pstatus  + studytime + failures  + activities + nursery  + internet + romantic + famrel + health + absences + edu + extrasup + at_home + services, nominal = ~ poly(traveltime,2) + schoolsup + poly(alc,3) + teacher,data = student_mat_final)

health_model_clm_PP <- clm(grade ~ higher + school + sex + age + address + famsize + Pstatus  + studytime + failures  + activities + nursery  + internet + romantic + famrel + goout + absences + edu + extrasup + at_home + services, nominal = ~ poly(traveltime,2) + schoolsup + poly(alc,3) + teacher,data = student_mat_final)

absences_model_clm_PP <- clm(grade ~ higher + school + sex + age + address + famsize + Pstatus  + studytime + failures  + activities + nursery  + internet + romantic + famrel + goout + health + edu + extrasup + at_home + services, nominal = ~ poly(traveltime,2) + schoolsup + poly(alc,3) + teacher,data = student_mat_final)

edu_clm_PP <- clm(grade ~ higher + school + sex + age + address + famsize + Pstatus  + studytime + failures  + activities + nursery  + internet + romantic + famrel + goout + health + absences  + extrasup + at_home + services, nominal = ~ poly(traveltime,2) + schoolsup + poly(alc,3) + teacher,data = student_mat_final)

alc_model_clm_PP <- clm(grade ~ higher + school + sex + age + address + famsize + Pstatus  + studytime + failures  + activities + nursery  + internet + romantic + famrel + goout + health + absences + edu + extrasup + at_home + services, nominal = ~ poly(traveltime,2) + schoolsup + teacher,data = student_mat_final)

extrasup_model_clm_PP <- clm(grade ~ higher + school + sex + age + address + famsize + Pstatus  + studytime + failures  + activities + nursery  + internet + romantic + famrel + goout + health + absences + edu  + at_home + services, nominal = ~ poly(traveltime,2) + schoolsup + poly(alc,3) + teacher,data = student_mat_final)

at_home_model_clm_PP <- clm(grade ~ higher + school + sex + age + address + famsize + Pstatus  + studytime + failures  + activities + nursery  + internet + romantic + famrel + goout + health + absences + edu + extrasup  + services, nominal = ~ poly(traveltime,2) + schoolsup + poly(alc,3) + teacher,data = student_mat_final)

services_model_clm_PP <- clm(grade ~ higher + school + sex + age + address + famsize + Pstatus  + studytime + failures  + activities + nursery  + internet + romantic + famrel + goout + health + absences + edu + extrasup + at_home, nominal = ~ poly(traveltime,2) + schoolsup + poly(alc,3) + teacher,data = student_mat_final)

teacher_model_clm_PP <- clm(grade ~ higher + school + sex + age + address + famsize + Pstatus  + studytime + failures  + activities + nursery  + internet + romantic + famrel + goout + health + absences + edu + extrasup + at_home + services, nominal = ~ poly(traveltime,2) + schoolsup + poly(alc,3),data = student_mat_final)


a1 <- anova(school_model_clm_PP,full_model_clm_PP) 
a2 <- anova(sex_model_clm_PP,full_model_clm_PP) 
a3 <- anova(age_model_clm_PP,full_model_clm_PP) 
a4 <- anova(address_model_clm_PP,full_model_clm_PP) 
a5 <- anova(famsize_model_clm_PP,full_model_clm_PP) 
a6 <- anova(Pstatus_model_clm_PP,full_model_clm_PP) 
a7 <- anova(traveltime_model_clm_PP,full_model_clm_PP) 
a8 <- anova(studytime_model_clm_PP,full_model_clm_PP) 
a9 <- anova(failures_clm_PP,full_model_clm_PP) 
a10 <- anova(schoolsup_clm_PP,full_model_clm_PP) 
a11 <- anova(activities_model_clm_PP,full_model_clm_PP) 
a12<- anova(nursery_model_clm_PP,full_model_clm_PP) 
a13 <- anova(higher_clm_PP,full_model_clm_PP) 
a14 <- anova(internet_clm_PP,full_model_clm_PP) 
a15 <- anova(romantic_clm_PP,full_model_clm_PP) 
a16 <- anova(famrel_clm_PP,full_model_clm_PP) 
a17 <- anova(goout_clm_PP,full_model_clm_PP) 
a18 <- anova(health_model_clm_PP,full_model_clm_PP) 
a19 <- anova(absences_model_clm_PP,full_model_clm_PP) 
a20 <- anova(edu_clm_PP,full_model_clm_PP) 
a21 <- anova(alc_model_clm_PP,full_model_clm_PP) 
a22 <- anova(extrasup_model_clm_PP,full_model_clm_PP)
a23 <- anova(at_home_model_clm_PP,full_model_clm_PP)
a24 <- anova(services_model_clm_PP,full_model_clm_PP)
a25 <- anova(teacher_model_clm_PP,full_model_clm_PP)


LR_stat <- c(a1$LR.stat[2],a2$LR.stat[2],a3$LR.stat[2],a4$LR.stat[2],a5$LR.stat[2],a6$LR.stat[2],a7$LR.stat[2],a8$LR.stat[2],a9$LR.stat[2],a10$LR.stat[2],a11$LR.stat[2],a12$LR.stat[2],a13$LR.stat[2],a14$LR.stat[2],a15$LR.stat[2],a16$LR.stat[2],a17$LR.stat[2],a18$LR.stat[2],a19$LR.stat[2],a20$LR.stat[2],a21$LR.stat[2],a22$LR.stat[2],a23$LR.stat[2],a24$LR.stat[2],a25$LR.stat[2])


pr_chisq <- c(a1$`Pr(>Chisq)`[2],a2$`Pr(>Chisq)`[2],a3$`Pr(>Chisq)`[2],a4$`Pr(>Chisq)`[2],a5$`Pr(>Chisq)`[2],a6$`Pr(>Chisq)`[2],a7$`Pr(>Chisq)`[2],a8$`Pr(>Chisq)`[2],a9$`Pr(>Chisq)`[2],a10$`Pr(>Chisq)`[2],a11$`Pr(>Chisq)`[2],a12$`Pr(>Chisq)`[2],a13$`Pr(>Chisq)`[2],a14$`Pr(>Chisq)`[2],a15$`Pr(>Chisq)`[2],a16$`Pr(>Chisq)`[2],a17$`Pr(>Chisq)`[2],a18$`Pr(>Chisq)`[2],a19$`Pr(>Chisq)`[2],a20$`Pr(>Chisq)`[2],a21$`Pr(>Chisq)`[2],a22$`Pr(>Chisq)`[2],a23$`Pr(>Chisq)`[2],a24$`Pr(>Chisq)`[2],a25$`Pr(>Chisq)`[2])


LR_table <- cbind(LR_stat,round(pr_chisq,5))
rownames(LR_table) <- c('school','sex','age' ,'address','famsize','Pstatus','traveltime','studytime','failures','schoolsup', 'activities' , 'nursery' , 'higher' , 'internet' , 'romantic' , 'famrel' , 'goout' , 'health' , 'absences' , 'edu' , 'alc' , 'extrasup' , 'at_home' , 'services' , 'teacher')

LR_table
```

**sex**, **age**,**studytime**,**failures**,**schoolsup**,**health**,**edu**,**alc**, and **teacher** are significant  with P <0.1. An interesting observation is the following fact: variable **alc** was clearly not significant in the proportional odds model, but it is significant in partial proportional odds model.
<br/>


```{r,message = FALSE,warning=FALSE}
no_alc_model_clm <- clm(grade ~ school + sex + age + address + famsize + Pstatus + traveltime + studytime + failures + schoolsup + activities + nursery + higher + internet + romantic + famrel + goout + health + absences + edu  + extrasup + at_home + services + teacher, data = student_mat_final)

anova(no_alc_model_clm,full_model_clm)
```

<br/>
We should mention that violating the proportional odds assumption might not be detrimental. The proportional odds model, even when the proportional odds assumption is not met, still retains meaning: it essentially estimates an average odds ratio across the outcome levels, i.e., it provides a general trend. Where a serious departure can occur is when investigating the effect on individual outcome levels; see https://www.fharrell.com/post/po/ for more details (and our **alc** example). 

Let us compare the predicted probabilities of proportional odds and partial proportional odds model. We will refit the partial proportional odds model using the package *VGAM*, since its *predict* provides all class probabilities.
<br/>

```{r,message = FALSE,warning=FALSE}

library(VGAM)
full_model_PP <- vglm(grade ~ higher + school + sex + age + address + famsize + Pstatus  + studytime + failures  + activities + nursery  + internet + romantic + famrel + goout + health + absences + edu + extrasup + at_home + services + poly(traveltime,2) + schoolsup + poly(alc,3) + teacher, family = cumulative(parallel = FALSE ~ poly(traveltime,2) + schoolsup + poly(alc,3) + teacher), data = student_mat_final)

# class probabilities
cbind(predict(full_model,type='probs')[1:10,], predict(full_model_PP,type = 'response')[1:10,])

# classified in the same category
sum(max.col(predict(full_model_PP,type = 'response')) == as.numeric(predict(full_model)))
```

<br/>
We observe that 86% (~340/395) of observations would be classified in the same category.

The last thing that we evaluate is residuals. Similarly to the logistic regression, it is not straightforward to define residuals that help to assess the model. For this purpose, we will use *surrogate residuals* proposed by  *D. Liu and H. Zhang. Residuals and diagnostics for ordinal regression models: a surrogate approach." Journal of the American Statistical Association 113.522 (2018): 845-854.* specifically for ordinal models (see https://koalaverse.github.io/sure/articles/sure.html for more details). Under a well-specified model, the surrogate residuals have a zero mean and are homoskedastic (i.e., have constant variance independent of $X$).  

Let us compute the surrogate residuals for our partial proportional odds model model using the *sure package* and plot them vs the linear predictor and variables we choose to omit.
<br/>

```{r,message = FALSE,warning=FALSE}
library(sure)

sres <- resids(full_model_clm_PP)

# QQ plot and residuals vs linear predictor
autoplot.resid(sres, what = 'qq') 
autoplot.resid(sres, what = 'fitted',fit = full_model_clm_PP) 

# residuals vs variables in the model
autoplot.resid(sres, what = 'covariate',x = student_mat_final$school) 
autoplot.resid(sres, what = 'covariate',x = student_mat_final$sex) 
autoplot.resid(sres, what = 'covariate',x = student_mat_final$age) 
autoplot.resid(sres, what = 'covariate',x = student_mat_final$address) 
autoplot.resid(sres, what = 'covariate',x = student_mat_final$famsize) 
autoplot.resid(sres, what = 'covariate',x = student_mat_final$Pstatus) 
autoplot.resid(sres, what = 'covariate',x = student_mat_final$traveltime) 
autoplot.resid(sres, what = 'covariate',x = student_mat_final$studytime) 
autoplot.resid(sres, what = 'covariate',x = as.factor(student_mat_final$failures))  
autoplot.resid(sres, what = 'covariate',x = student_mat_final$schoolsup) 
autoplot.resid(sres, what = 'covariate',x = student_mat_final$activities) 
autoplot.resid(sres, what = 'covariate',x = student_mat_final$nursery) 
autoplot.resid(sres, what = 'covariate',x = student_mat_final$higher) 
autoplot.resid(sres, what = 'covariate',x = student_mat_final$internet) 
autoplot.resid(sres, what = 'covariate',x = student_mat_final$romantic)
autoplot.resid(sres, what = 'covariate',x = student_mat_final$famrel)
autoplot.resid(sres, what = 'covariate',x = student_mat_final$goout)
autoplot.resid(sres, what = 'covariate',x = student_mat_final$health)
autoplot.resid(sres, what = 'covariate',x = student_mat_final$absences)
autoplot.resid(sres, what = 'covariate',x = student_mat_final$edu)
autoplot.resid(sres, what = 'covariate',x = student_mat_final$alc)
autoplot.resid(sres, what = 'covariate',x = student_mat_final$extrasup)
autoplot.resid(sres, what = 'covariate',x = student_mat_final$at_home)
autoplot.resid(sres, what = 'covariate',x = student_mat_final$services)
autoplot.resid(sres, what = 'covariate',x = student_mat_final$teacher)

# residuals vs omitted variables
autoplot.resid(sres, what = 'covariate',x = student_mat$Medu)
autoplot.resid(sres, what = 'covariate',x = student_mat$Fedu)
autoplot.resid(sres, what = 'covariate',x = student_mat$Mjob)
autoplot.resid(sres, what = 'covariate',x = student_mat$Fjob)
autoplot.resid(sres, what = 'covariate',x = student_mat$Dalc)
autoplot.resid(sres, what = 'covariate',x = student_mat$Walc)
autoplot.resid(sres, what = 'covariate',x = student_mat$famsup)
autoplot.resid(sres, what = 'covariate',x = student_mat$paid)
autoplot.resid(sres, what = 'covariate',x = student_mat$guardian)
autoplot.resid(sres, what = 'covariate',x = student_mat$freetime)
autoplot.resid(sres, what = 'covariate',x = student_mat$reason)
```

<br/>
We see no apparent trends in the residuals. However, the question always is how much obvious trends in the residuals are "obvious." For illustration, let us examine some residual plots for the *obviously* wrong trivial model. 
<br/>

```{r,message = FALSE,warning=FALSE}
null_model <- polr(grade ~ 1, data = student_mat_final)
sres_null <- resids(null_model)
autoplot.resid(sres_null, what = 'qq') 
autoplot.resid(sres_null, what = 'fitted',fit = full_model) 

autoplot.resid(sres_null, what = 'covariate',x = student_mat_final$sex) 
autoplot.resid(sres_null, what = 'covariate',x = student_mat_final$age) 
autoplot.resid(sres_null, what = 'covariate',x = student_mat_final$famsize) 
autoplot.resid(sres_null, what = 'covariate',x = student_mat_final$edu)
autoplot.resid(sres_null, what = 'covariate',x = as.factor(student_mat_final$failures)) 
autoplot.resid(sres_null, what = 'covariate',x = student_mat_final$studytime)
autoplot.resid(sres_null, what = 'covariate',x = student_mat_final$health)
```

<br/>
We observe that some trends (e.g., for **failures** and **edu**) are quite noticeable, suggesting that these variables should be included in the model. 
<br/>

## Continuation ratio model

<br/>
Before we conclude the first part of this project, we will take a brief look at an alternative models of ordinal response. We start with the *continuation ratio model*. Unlike the proportional odds model, the continuation ratio (CR) model considers conditional probabilities $P[Y = k | Y \geq k ] = \mathrm{ilogit}\, (\theta_k + X\beta)$. Essentially, the CR model is a
discrete version of the Cox proportional hazards model used in survival analysis.

A nice property of the CR model is that it can be estimated using the logistic regression by an appropriate extension of the design matrix (*F. E. Harrell. Regression modeling strategies: with applications to linear models, logistic regression, and survival analysis. Vol. 608. New York: springer, 2001.*). In R, this approach is implemented in the *rms* package as follows.
<br/>


```{r,message = FALSE,warning=FALSE}
u <- cr.setup(grade)
student_mat_expanded <- student_mat_final[u$sub , ]
y <- u$y
cohort <- u$cohort
levels(cohort) <- c('>= F','>= C/D')
```

<br/>
The CR model is then a logistic regression of *y* against the *cohort* variable (corresponding to $\theta_k$ in the continuation ratio model) and the original regressors.
<br/>

```{r,message = FALSE,warning=FALSE}
cr_model <- glm(y ~ cohort + school + sex + age + address + famsize + Pstatus + traveltime + studytime + failures + schoolsup + activities + nursery  + higher  + internet  + romantic + famrel + goout + health + absences + edu + alc +  extrasup + at_home + services + teacher - 1, data = student_mat_expanded, family = binomial)
anova(cr_model)
```

<br/>
We see that predictors that appear to be significant (**sex**, **age**, **address**,**studytime**, **failures**, **schoolsup**, **higher**,**health**,**teacher**) are very similar to those in the proportional odds model.

Computing class probabilities is a bit more involved, requiring the derivation of unconditional probabilities from the conditional ones.
<br/>

```{r}
# probability of F
prob_F <- ilogit(model.matrix(full_model)[,2:44] %*%  coefficients(cr_model)[3:45] + coefficients(cr_model)[1])
resF <- cbind(prob_F,predict(full_model, type = 'probs')[,1])
colnames(resF) <- c('CR','PO')
resF[1:15,]

# probability of C/D
prob_CD <- (ilogit(model.matrix(full_model)[,2:44] %*%  coefficients(cr_model)[3:45] + coefficients(cr_model)[2]))*
(1-prob_F)
resCD <- cbind(prob_CD,predict(full_model, type = 'probs')[,2])
colnames(resCD) <- c('CR','PO')
resCD[1:15,]

# probability of A/b
prob_AB <- (1-prob_CD-prob_F)
resAB <- cbind(prob_AB,predict(full_model, type = 'probs')[,3])
colnames(resAB) <- c('CR','PO')
resAB[1:15,]
```

<br/>
We observe that the predicted probabilities generally align with those provided by the ordered logit model. We can check how much the predicted classes differ overall.
<br/>

```{r}
pred_cr <- cbind(resF[,1],resCD[,1],resAB[,1])
sum(max.col(pred_cr) == as.numeric(predict(full_model)))
```

<br/>
We see that the predicted classes are the same in 93% (~369/395) of all cases. We should note that an alternative method for fitting the CR model is available via the *VGAM* package.
<br/>

```{r,message = FALSE,warning=FALSE}
library(VGAM)
cr_vglm <- vglm(grade ~ school + sex + age + address + famsize + Pstatus + traveltime + studytime + failures + schoolsup + activities + nursery  + higher  + internet  + romantic + famrel + goout + health + absences + edu + alc +  extrasup + at_home + services + teacher, data = student_mat_final, family=cratio(parallel = TRUE))

# same coefficients (just opposite signs)
coefficients(cr_vglm)
coefficients(cr_model)

# same predicted probabilities
predict(cr_vglm,type = 'response')[1:15,]
pred_cr[1:15,]
```

<br/>
The CR model is not inherently better than the proportional odds model. However, its main advantage over the proportional odds model is the fact that its "parallel slopes assumption" ($\beta$ does not depend on class) is much more easily relaxed than in the proportional odds model (which requires specialized software). One just needs to include interaction terms with the *cohort* in the logit model (*F. E. Harrell. Regression modeling strategies: with applications to linear models, logistic regression, and survival analysis. Vol. 608. New York: springer, 2001.*). 
<br/>

```{r,message = FALSE,warning=FALSE}
cr_model_inter <- glm(y ~ cohort*(school + sex + age + address + famsize + Pstatus + traveltime + studytime + failures + schoolsup + activities + nursery  + higher  + internet  + romantic + famrel + goout + health + absences + edu + alc +  extrasup + at_home + services + teacher) - 1, data = student_mat_expanded, family = binomial)

anova(cr_model,cr_model_inter)
```

<br/>
We observe that the difference between the two CR models is not significant, which suggests that the parallel slopes assumption in the CR model is overall somewhat justified. 
<br/>

## Adjacent categories model 

<br/>
The next ordinal response model we will discuss here is an adjacent categories model. The probabilities in the model are given as $\mathrm{log} \; \frac{p_k}{p_{k+1}} = \theta_k + X\beta$. The adjacent categories mode can be fitted via the *VGAM* package.
<br/>

```{r,message = FALSE,warning=FALSE}
ac_vglm <- vglm(grade ~ school + sex + age + address + famsize + Pstatus + traveltime + studytime + failures + schoolsup + activities + nursery  + higher  + internet  + romantic + famrel + goout + health + absences + edu + alc +  extrasup + at_home + services + teacher, data = student_mat_final, family=acat(parallel = TRUE))
```

<br/>
Let us check the significant predictors.
<br/>

```{r,message = FALSE,warning=FALSE}
Anova(ac_vglm)
```

<br/>
The significant predictors are **sex**,**age**, **studytime**, **failures**, **schoolsup**, **health**, **edu**, and **teacher**, which is similar to the proportional odds model and the continual ratio model. We can again compare the predictions of the model
<br/>

```{r}
sum(max.col(predict(ac_vglm ,type='response')) == as.numeric(predict(full_model)))
```

<br/>
We see that about 96% (~380/395) of the observations has the same predicted class as in the proportional odds model. Again, we could consider the model with non-parallel slopes. We could suspect based on our observations for the proportional odds model that **alc** violates the parallel slopes condition.
<br/>

```{r,message = FALSE,warning=FALSE}
ac_vglm_alc_PP <- vglm(grade ~ school + sex + age + address + famsize + Pstatus + traveltime + studytime + failures + schoolsup + activities + nursery  + higher  + internet  + romantic + famrel + goout + health + absences + edu + alc +  extrasup + at_home + services + teacher, data = student_mat_final, family=acat(parallel = FALSE ~ alc))
anova(ac_vglm,ac_vglm_alc_PP,type = 'I')
```

<br/>
Indeed, it seems that it does. Again, we could then observe that in the non-parallel model **alc** is somewhat significant whereas in the plain adjacent categories model it was not. 
<br/>

```{r}
ac_vglm_noalc <- vglm(grade ~ school + sex + age + address + famsize + Pstatus + traveltime + studytime + failures + schoolsup + activities + nursery  + higher  + internet  + romantic + famrel + goout + health + absences + edu +  extrasup + at_home + services + teacher, data = student_mat_final, family=acat(parallel = TRUE))
anova(ac_vglm_noalc,ac_vglm_alc_PP,type = 'I')
```

<br/>
Overall, we could repeat all the steps for the adjacent categories model as we did for the proportional odds model.
<br/>

## Multinomial logistic regression

<br/>
The last model that should be mentioned here is the multinomial logit model. This model assumes nominal categorical response, i.e, it assumes that there is no particular order for resposne categories. In other words, it corresponds to the model in which there are no "parallel" terms. Thus, this model has the largest number of parameters. The model is described by the equations $\mathrm{log}\; \frac{p_i}{p_1} = X\beta_i$, where $i = 2,3,\ldots$.
<br/>

```{r,message = FALSE,warning=FALSE}
multi_model <- vglm(grade ~ school + sex + age + address + famsize + Pstatus + traveltime + studytime + failures + schoolsup + activities + nursery + higher + internet + romantic + famrel + goout + health + absences + edu + alc + extrasup + at_home + services + teacher, family=multinomial,data = student_mat_final)
multi_model
```

<br/>
We can again check the significant predictors.
<br/>

```{r,message = FALSE,warning=FALSE}
Anova(multi_model)
```

<br/>
The significant predictors (P <0.1) are **sex**,**age**, **studytime**, **failures**, **schoolsup**,**health**, and **teacher**, which is similar to the other models we have considered. We can also compare the predictions of the model
<br/>

```{r}
sum(max.col(predict(multi_model ,type='response')) == as.numeric(predict(full_model)))
```

<br/>
We see that about 73% (~289/395) of the observations has the same predicted class as in the proportional odds model. Thus, these models are quite different. The multinomial logit is the most flexible model we have considered but it has about two times the number of parameters than the proportional odds model and thus, there is the greatest risk of overfitting and the greatest risk of poor generalization to new data.

We will conclude Part One here. In the second part of this demonstration, we examine the predictive performance of our four models and discuss the results. 
<br/>

